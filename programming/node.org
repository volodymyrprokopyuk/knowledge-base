* Node.js
** Reactor pattern
*** Reactor and SED

- Sync Event Demultiplexer SED :: SED provides an OS-level =syscall= for async,
  non-blocking IO operations e.g. =epoll= on Linux. SED watches sync for a set
  of async operations to complete and allows multiple async operations to be
  processed in a single thread (events demultiplexing)
- Reactor pattern :: The reactor pattern async executes a handler (app callback)
  for each async operation (non-blocking IO)
    - App requests async operations ~{ resource: file, operation: read, handler
      callback }~ at *SED* (non-blocking)
    - SED watches requested async operations for completion (blocking)
    - SED enqueues ~{ event: operation, handler: callback }~ to the *event queue
      EQ*
    - *Single-threaded event loop EL* reads the EQ and executes handlers (app
      callbacks) to completion (no race conditions). App callbacks request more
      async operations at SED
    - When a callback requests a new async operations it gives control back to
      the event loop. *Invoking an async operation* always *unwinds the stack
      back to the event loop*, leaving it free to handle other requests
      (IO-bound)
    - EL blocks again (next EL cycle) at SED for new async operations to
      complete

*** Node.js and libuv

- libuv :: SED + reactor (event queue + event loop) is a cross-platform
  low-level IO engine for Node.js
    - High resolution clock and timers
    - Async thread pool and communication channels
    - Async child processes and signal handling
    - Async FS, TCP/UDP, DNS
    - Async IPC via shared UNIX domain sockets (bidirectional channels)
- Node.js :: Node.js provides async IO operations in parallel by libuv threads,
  sync callback code to completion in the event loop. Node.js = libuv (SED +
  reactor) + V8 (JavaScript runtime)
- libuv IO concurrency :: In Node.js only async, non-blocking IO operations are
  *executed in parallel* by the *libuv internal IO threads*. Sync code inside
  callbacks (until next async operation) is *executed to completion without race
  conditions* by the *single-threaded event loop*
- Node.js scalability :: Node.js scales well (high throughput) when serving
  *large number of clients* with a *small number of threads*: the main thread
  running the event loop, libuv aync IO threads, CPU-intensive Worker threads.
  Node.js scales well when processing *small async IO tasks* and *small
  CPU-intensive tasks*.

*** Big task partitioning

- Big task partitioning :: Partition big tasks into smaller sub-tasks to be
  processed concurrently and *minimize variation* of tasks to give fair amount
  of time to every client. Alternatively, offload CPU-intensive tasks to *Worker
  threads* paying *communication cost of serialization and de-serialization*. IO
  threads are blocked waiting for async IO operations to complete. Worker
  threads are executed in parallel on CPU cores
  #+BEGIN_SRC js
function sumFirstN(n) { // large blocking task
  let sum = 0
  for (let i = 0; i <= n; i++) {
    sum += i
  }
  return sum
}
console.log(sumFirstN(10)) // 55
function sumFirstNCb(n, size, done) { // partitioned small tasks (callback)
  let sum = 0, i = 1
  function chunk() {
    let j = 0
    while (j++ < size && i <= n) {
      sum += i++
    }
    i > n ? done(null, sum) : queueMicrotask(chunk) // always async
  }
  queueMicrotask(chunk) // start async chunk
}
sumFirstNCb(10, 2, (err, sum) => console.log(sum)) // 55
function sumFirstNP(n, size) { // partitioned small tasks (Promise)
  return new Promise(resolve => {
    let sum = 0, i = 1
    function chunk() {
      let j = 0
      while(j++ < size && i <= n) {
        sum += i++
      }
      i > n ? resolve(sum) : queueMicrotask(chunk) // always async
    }
    queueMicrotask(chunk) // start async chunk
  })
}
console.log(await sumFirstNP(10, 2)) // 55
  #+END_SRC

** Callback pattern
*** Callback and CPS

- Callback :: A callback is a *first-class function* (to pass a callback) + a
  *closure* (to retain a caller context) that is passed to a non-blocking async
  function ~f(args, cb)~ that *returns immediately*, and *is executed on
  completion* of the requested async operation. A callback implements the
  *Continuation-Passing Style*, CPS = control is passed explicitly in a form of
  a continuation (callback), ~return/throw~ => ~callback(error, result)~
- Callback trust issues :: A callback *is expected to be invoked exactly once*
  either with a result or an error e.g. timeout. A callback has *trust issues*:
  a caller passes callback code to a third party, an async function receives
  callback code from a third party
- Callback hell :: A callback hell represents a deeply nested code as a result
  of *in-place nested anonymous callbacks* that unnecessary consume memory
  because of closures
  - Do not abuse in-place nested anonymous callbacks
  - *Create named callbacks* with clearly defined interface (and without
    unnecessary closures that consume memory)

*** Error handling

- Error propagation :: An async result or an error is *propagated through a
  chain* of nested ~callbacks(error | null, data | cb(data))~ that accept an
  ~error | null~ *first*, then a ~result | cb(data)~ that comes always *last*.
  Never =return= a result or =throw= an error from a callback. =return= and
  =throw= are unusable from a callback
- Uncaught error :: An uncaught error thrown from an async function *propagates
  to the stack of the event loop* (not to the next callback, not to the stack of
  the caller that triggered the async operation).
  ~process.on(unchaughtException)~ is emitted and the *process exits* with a
  non-zero exit code

*** Event loop blocking

- Even loop blocking :: *Sync CPU-bound operations* in a callback *block the
  event loop* and put on hold all concurrent processing blocking the whole
  application
  - *Interleave* each step of a CPU-bound sync operation with a
    ~queueMicrotask()~ or ~setImmediate()~ to let other IO tasks to be processed
    by the event loop. The interleaving is not efficient due to context
    switching, more complex interleaving algorithms
  - Child process :: Use a pool of reusable external Node.js processes
    ~child_process.fork()~ with a communication channel
    ~process/worker.send().on(message)~ leaves the event loop unblocked. Child
    processes are more efficient as they run to completion in parallel and use
    unmodified algorithms
  - Worker thread :: ~new Worker(module, opts)~ see below

*** Convert sync to async

- Convert sync to async :: Avoid mixing sync code with async code under the same
  interface. Convert sync code to an async callback
  - Next tick queue :: The ~process.nextTick()~ is managed by *Node.js*, is
    executed just after the current operation, before other IO tasks in *the
    same cycle* of the event loop
  - Micro task queue :: The ~queueMicrotask()~ is managed by *V8*, is used to
    *execute Promise* handlers ~.then()~, ~.catch()~, and ~.finally()~. The
    microtask queue is drained immediately after the next tick queue is drained
    within *the same cycle* of the event loop
    #+BEGIN_SRC js
import { EventEmitter } from "node:events"
class EE extends EventEmitter {
  constructor() {
    super()
    // emits sync before a listener is attached (does not work)
    this.emit("ready", 1)
    // emits async after a listener is attached (works)
    queueMicrotask(() => this.emit("ready", 2))
  }
}
const ee = new EE()
ee.on("ready", console.log) // 2
    #+END_SRC
  - Macro task queue :: A macro task queue is always executed in *the next
    cycle* of the event loop
    - Immediate task :: The ~setImmediate()~ is executed as soon as possible on
      the next cycle of the event loop, before any timers ~setTimeout()~ and
      ~setInterval()~
    - Delayed task :: The ~setTimeout()~ is executed after a delay in one of
      the next cycles of the event loop

*** Sequential execution

- Sequential execution :: A sequential execution executes async tasks in
  sequence one task at a time until all tasks successfully complete or until the
  first error
  #+BEGIN_SRC js
function task(v, done) {
  setTimeout(() => {
    console.log(v)
    done(null, v + 1)
  }, 500)
}
function error(v, done) {
  setTimeout(() => done(new Error("oh")), 500)
}
function sequence(tasks, done) {
  let i = 0;
  function cb(err, v) {
    if (err) {
      done(err) // stop the sequence on the first error
    } else {
      if (i < tasks.length) {
        // continue the sequence with the next async task
        queueMicrotask(() => tasks[i++](v, cb))
      } else {
        done(null, v) // complete the sequence with the last async result
      }
    }
  }
  if (tasks.length === 0) {
    queueMicrotask(() => done(null, 0))
  } else {
    // start the sequence with the first async task
    queueMicrotask(() => tasks[i++](1, cb))
  }
}
sequence([task, task, task], console.log) // 1, 2, 3, null, 4
sequence([task, task, error, task], console.log) // 1, 2, oh
sequence([], console.log) // null, 0
  #+END_SRC

*** Parallel execution

- Parallel execution :: A parallel execution executes all async tasks in
  parallel (unlimited) until all tasks successfully complete or until the first
  error
  #+BEGIN_SRC js
function parallel(tasks, done) {
  let completed = 0
  function cb(err, v) {
    if (err) {
      done(err) // stop the parallel on the first error
    } else {
      if (++completed === tasks.length) {
        done(null, v) // complete the parallel with the last async result
      }
    }
  }
  if (tasks.length === 0) {
    queueMicrotask(() => done(null, 0))
  } else {
    for(const task of tasks) {
      // start all async tasks at once in parallel
      queueMicrotask(() => task(1, cb))
    }
  }
}
parallel([task, task, task], console.log) // 1, 1, 1, null, 2
parallel([task, task, error, task], console.log) // 1, 1, oh, 1
parallel([], console.log) // null, 0
  #+END_SRC

*** Limited parallel execution

- Limited parallel execution :: A limited parallel execution executes at most N
  async tasks in parallel (limited) until all tasks successfully complete or the
  first error
  #+BEGIN_SRC js
function parallelLimit(tasks, limit, done) {
  let i = 0, running = 0, completed = 0
  function cb(err, v) {
    if (err) {
      done(err) // stop the parallel on the first error
    } else {
      running--
      if (++completed === tasks.length) {
        done(null, v) // complete the parallel with the last async result
      } else {
        runLimit(v) // start up to limit async tasks in parallel
      }
    }
  }
  function runLimit(v) {
    while (running < limit && i < tasks.length) {
      running++
      queueMicrotask(() => tasks[i++](v, cb))
    }
  }
  if (tasks.length === 0) {
    queueMicrotask(() => done(null, 0))
  } else {
    runLimit(1) // start up to limit async tasks in parallel
  }
}
// 1, 1; 2, 2; 3, null, 4
parallelLimit([task, task, task, task, task], 2, console.log)
// 1, 1; 2, oh; 3
parallelLimit([task, task, task, error, task], 2, console.log)
parallelLimit([], 2, console.log) // null, 0
  #+END_SRC

** Observer pattern

*** Event emitter

- Observer :: An =EventEmitter= continuously *notifies multiple observers*
  subscribed with listeners on state changes through different types of
  recurrent events. =EventEmitter= *does not have trust issues* as the callback
  is controlled by a caller
- Async events :: Events are *guaranteed to fire async* in the next cycle of the
  event loop. However, ~ee.emit()~ is called sync for all registered listeners
- Event emitter :: =EventEmitter= registers multiple ~function listener(event,
  ...args)~ for specific event types ~ee.on|once(event, listener)~.
  Synchronously ~ee.emit(event, ...args)~ for all registered listeners.
  ~ee.removeListener(event, listener)~ unsubscribes a listeners when it is no
  longer needed to prevent memory leaks due to captured context in listener
  closures

*** Error handling

- Result and error propagation :: Async result and error is propagated through
  ~emit~ events. Never ~return~ a result or ~throw~ an error from an
  ~EventEmitter~. ~return~ and ~throw~ are unusable with ~EventEmitter~
- Error handling :: If ~ee.on(error, ...args)~ is not registered an ~Error~ is
  thrown. Always register a listener for the ~error~ event
  #+BEGIN_SRC js
import { EventEmitter } from "node:events"
class EE extends EventEmitter {}
const ee = new EE()
ee.on("error", console.error)
ee.on("start", console.log)
ee.on("start", () => ee.emit("error", new Error("oh")))
setTimeout(() => ee.emit("start", 1), 100) // 1, oh
  #+END_SRC

** Promise pattern
*** Sequential execution

- Sequential execution ::
  #+BEGIN_SRC js
function task(v) {
  return new Promise(resolve =>
    setTimeout(() => {
      console.log(v)
      resolve(v + 1)
    }, 500)
  )
}
function error() {
  return new Promise((_, reject) =>
    setTimeout(() => reject(new Error("oh")), 500)
  )
}
function sequence(tasks) {
  if (tasks.length === 0) {
    return Promise.resolve(0)
  }
  let res = Promise.resolve(1)
  for (const task of tasks) {
    res = res.then(task) // dynamically build a chain of promises
  }
  return res
}
sequence([task, task, task]).then(console.log) // 1, 2, 3, 4
sequence([task, task, error, task]).then(console.log) // 1, 2, oh
sequence([]).then(console.log) // 0
  #+END_SRC

*** Parallel execution

- Parallel execution ::
  #+BEGIN_SRC js
function parallel(tasks) {
  const res = Array(tasks.length)
  if (tasks.length === 0) {
    return Promise.resolve(res)
  }
  let completed = 0
  return new Promise((resolve, reject) => {
    function done(i) {
      return function(v) {
        res[i] = v // record a promise result
        if (++completed == tasks.length) {
          resolve(res) // complete the parallel with results of all promises
        }
      }
    }
    for (let i = 0; i < tasks.length; i++) {
      // start all promises at once
      // stop the parallel on the first rejection
      tasks[i](1).then(done(i), reject)
    }
  })
}
parallel([task, task, task]).then(console.log) // 1, 1, 1, [2, 2, 2]
parallel([task, task, error, task]).then(console.log) // 1, 1, 1, oh
parallel([]).then(console.log) // []
  #+END_SRC

*** Limited parallel execution

- Limited parallel execution ::
  #+BEGIN_SRC js
function parallelLimit(tasks, limit) {
  const res = Array(tasks.length)
  if (tasks.length === 0) {
    return Promise.resolve(res)
  }
  let i = 0, running = 0, completed = 0
  return new Promise((resolve, reject) => {
    function done(i) {
      return function(v) {
        res[i] = v
        running--
        if (++completed === tasks.length) {
          resolve(res)
        } else {
          runLimit(v)
        }
      }
    }
    function runLimit(v) {
      while (running < limit && i < tasks.length) {
        running++
        tasks[i](v).then(done(i), reject)
        i++
      }
    }
    runLimit(1)
  })
}
// 1, 1; 2, 2; 3, [2, 2, 3, 3, 4]
parallelLimit([task, task, task, task, task], 2).then(console.log)
// 1, 1; 2, oh
parallelLimit([task, task, task, error, task], 2).then(console.log)
parallelLimit([], 2).then(console.log) // []
  #+END_SRC

** Async/await pattern
*** Async function

- Async function :: An =async= function *always returns a Promise* immediately
  and synchronously. Async function is a *promise-yielding generator* that *on
  each await expression yields a promise* and suspends the generator. The
  internal state of the generator is preserved and *control is returned to the
  event loop*. When the promise that has been awaited settles, control is given
  back to the async function and the generator resumes
- async/await and Promise :: The =Promise= abstraction and the ~async/await~
  syntax is used to manage *async operations in a sync-like syntax*.
- await sequencing :: =await= sequencing introduces unnecessary blocking for
  unrelated operations. Use =Promise.all()= instead
  #+BEGIN_SRC js
console.log(await task(1), await task(2)) // sequence, slow
console.log(await Promise.all([task(1), task(2)])) // parallel, fast
  #+END_SRC

*** Error handling

- return await, yield await :: ~try/catch/throw~ inside an async function or a
  generator works for both sync and async code. Use =return await= or =yield
  await= to *prevent errors on the caller side* and *catch errors locally*
  within the async function or a generator
  #+BEGIN_SRC js
async function localError() {
  try {
    return Promise.reject(new Error("oh")) // Caller error oh
    return await Promise.reject(new Error("oh")) // Local error oh
  } catch (err) {
    console.error("Local error", err)
  }
}
localError().catch(err => console.error("Caller error", err))
  #+END_SRC

- *Sequential execution* (sync loop with await)
  #+BEGIN_SRC js
    async function sequence(tasks) {
      for (const task of tasks) { await task() }
    }
    // 1, 2, 3, undefined
    const tasks = [1, 2, 3].map(el => () => task(el))
    // 1, 2, oh
    const tasks = [1, 2, 0, 3].map(el => () => task(el))
    try { console.log(await sequence(tasks)) }
    catch (err) { console.error(err.message) }
    console.log(await sequence([])) // undefined
  #+END_SRC
- *Parallel execution* (await loop)
  #+BEGIN_SRC js
    // Use the prallel() from the Promise pattern above
    // 1, 2, 3, undefined
    const tasks = [1, 2, 3].map(el => () => task(el))
    // 1, 2, oh, 3
    const tasks = [1, 2, 0, 3].map(el => () => task(el))
    try { console.log(await parallel(tasks)) }
    catch (err) { console.error(err.message) }
    console.log(await parallel([])) // undefined
  #+END_SRC
- *Limited parallel execution*
  #+BEGIN_SRC js
    // Use the parallelLimit() from the Promise pattern above
    // 1, 2 | 3, 4 | 5, undefined
    const tasks = [1, 2, 3, 4, 5].map(el => done => task(el, done))
    // 1, 2, | 3, oh | 4
    const tasks = [1, 2, 3, 0, 4].map(el => done => task(el, done))
    try { console.log(await parallelLimit(tasks, 2)) }
    catch (err) { console.error(err.message) }
    console.log(await parallelLimit([], 2)) // undefined
  #+END_SRC

** Stream pattern

- *Streaming* = parallel staged processing of data in chunks as soon as it
  arrives with internal buffering and *backpressure* (reactive, modular,
  composable, *constant memory*, short GC cycles). `Stream` is an abstraction
  on top of a data source `Readable`, a data transformation `Transform`, and a
  data sink `Writable`. `Stream` extends `EventEmitter`
    - *Binary mode* `Buffer` or string with an encoding for IO processing
    - *Object mode* JavaScript object/array for function composition
- `Writable` = standard abstraction of a *data sink* on top of an underlying
  resource with *backpressure* when an internal buffer has exceeded the
  `highWaterMark` then `.write(chunk) => false` stop writing until a Writable
  notifies when the underlying resource is ready for writing `.on(drain)` to
  resume writing
    #+BEGIN_SRC js
    import { Writable } from "node:stream"
    import { finished } from "node:stream/promises"
    class Sink extends Writable {
      // allocates resources
      _construct(done) { this.buffer = []; done(null) }
      _write(chunk, encoding, done) { // if error done(new Error("oh"))
        setTimeout(() => { this.buffer.push(chunk); done(null) }, 100)
      }
      // flushes buffered data before a Writable end
      _final(done) { this.buffer = this.buffer.join(""); done(null) }
      // disposes resources
      _destroy(err, done) { this.buffer += "."; done(err) }
    }
    const sink = new Sink()
    sink.write("a"); sink.write("b"); sink.end("c")
    await finished(sink)
    console.log(sink.buffer) // abc.
    #+END_SRC
- `Readable` = standard abstraction of a *data source* from an underlying
  resource with *backpressure* when `this.push(chunk) => false` stop reading
  from the underlying resource. The `_read(size)` will be automatically called
  later to read more data from the underlying resource. A Readable will start
  reading from the underlying resource only when data consumption begins
    #+BEGIN_SRC js
    import { Readable } from "node:stream"
    class Source extends Readable {
      constructor(source, { encoding = "utf8", ...opts } = { }) {
        super({ encoding, ...opts })
        this.source = source; this.i = 0
      }
      // allocates resources
      _construct(done) { this.source = this.source.split(" "); done(null) }
      _read(size) {
        setTimeout(() => // if error this.destroy(new Error("oh"))
          this.i < this.source.length ? this.push(this.source[this.i++]) :
            this.push(null), 100 // end of a Readable
        )
      }
      // disposes resources
      _destroy(err, done) { this.source = null; done(err) }
    }
    #+END_SRC
- *Async iterator* `for await ... of` to consume a Readable
    #+BEGIN_SRC js
    const source = new Source("a b c")
    for await (const chunk of source) { console.log(chunk) } // a b c
    #+END_SRC
- *Flowing mode* (push) = a Readable *producer pushes* data to a consumer
  as soon as it is available. The flowing mode is activated by `.on(data)`,
  `.pipe(writable)`, `.resume()`
    #+BEGIN_SRC js
    const source = new Source("a b c")
    source.on("data", chunk => console.log(chunk))
    source.on("end", () => console.log(".")) // a b c .
    #+END_SRC
- *Paused mode* (pull) = a *consumer pulls* data from a Readable producer in
  a controlled way. The paused mode is activated by `.on(readable)`,
  `.unpipe(writable)`, `.pause()`
    #+BEGIN_SRC js
    const source = new Source("a b c")
    source.on("readable", () => {
      let chunk
      while(chunk = source.read()) { console.log(chunk) }
    })
    source.on("end", () => console.log(".")) // a b c .
    #+END_SRC
- *Piping* = `readable.pipe(duplex | writable)` creates a *chain of
  streams*, switches a Readable producer to the *flowing mode*, returns the
  last stream in a chain. A `.pipe()` *controls backpressure automatically*.
  Errors are not propagated automatically through a `pipe()`, `on(error)`
  handlers must be attached to every step. Destruction of a pipeline constructed
  with a `.pipe()` has to be performed manually by calling `.destroy()` on every
  step. Multiple Writables can be attached to the same Readable
    #+BEGIN_SRC js
    const source = new Source("a b c"),
          sink = new Sink(), sink2 = new Sink()
    source.pipe(sink); source.pipe(sink2)
    await finished(sink); await finished(sink2)
    console.log(sink.buffer, sink2.buffer) // abc. abc.
    #+END_SRC
- `Duplex` = *independent* Readable `_read(size)` and Writable `_write(chunk,
  encoding, done)`, `_final(done)` for stream chaining through
  `readable.pipe(duplex | writable)` or `pipeline(readable, ...transform,
  writable)`
- `Transform` = a Readable *dependent* on a Writable through a *composable
  transformation* that follows a pattern Writable => Transform => Readable.
  Multiple Transforms can be chained to produce a new Transform
    #+BEGIN_SRC js
    import { Transform } from "node:stream"
    class Double extends Transform {
      _transform(chunk, encoding, done) { // Writable side
        setTimeout(() => { // if error done(new Error("oh"))
          const trans = String(chunk).split("").map(ch => ch + ch).join("")
          this.push(trans); done(null) // Readable side
        }, 100)
      }
      _flush(done) { this.push("-"); done(null) } // before a Readable end
    }
    class Upcase extends Transform {
      _transform(chunk, encoding, done) {
        setTimeout(() => {
          this.push(String(chunk).toUpperCase()); done(null)
        }, 100)
      }
      _flush(done) { this.push("_"); done(null) }
    }
    const source = new Source("a b c"), double = new Double(),
          upcase = new Upcase(), sink = new Sink()
    await finished(source.pipe(double).pipe(upcase).pipe(sink))
    console.log(sink.buffer) // AABBCC-_.
    #+END_SRC
- `pipeline(readable, ...transform, writable)` = combines streams in a
  *non-composable* end-to-end pipeline that follows a pattern Readable =>
  ...Transform => Writable, *automatically handles backpressure, error
  propagation, and destruction of streams* on pipeline success or failure
    #+BEGIN_SRC js
    import { pipeline } from "node:stream/promises"
    const source = new Source("a b c"), upcase = new Upcase(),
          double = new Double(), sink = new Sink()
    await pipeline(source, double, upcase, sink)
    console.log(sink.buffer) // AABBCC-_.
    #+END_SRC
- `compose(...streams)` = combines streams in a new *composable* Duplex stream
  that follows a pattern Writable => ...Transform => Readable using `pipeline()`
    #+BEGIN_SRC js
    import { compose } from "node:stream"
    const source = new Source("a b c"), upcase = new Upcase(),
          double = new Double(), sink = new Sink()
    await pipeline(source, compose(double, upcase), sink)
    console.log(sink.buffer) // AABBCC-_.
    #+END_SRC
- *Async iterator* `for await ... of` `next() => Promise()` and
  *async generator* `async function* { yield Promise() }` form a basis for
  *language-level construction of streams*. A composable stream Writable =>
  Readable is an async generator that internally uses an async iterator to
  consume an input Readable
    #+BEGIN_SRC js
    async function* upcase(readable) { // async generator
      for await (const ch of readable) { // async iterator
        yield new Promise(resolve => // async generator
          setTimeout(() => resolve(ch.toUpperCase()), 100)
        )
      }
    }
    const source = new Source("a b c"), sink = new Sink()
    await pipeline(source, upcase, sink)
    console.log(sink.buffer) // ABC.
    #+END_SRC

** Child process

- `spawn(cmd, args, opts): ChildProcess` executes a *command from a PATH*.
  Async foundation for all tools below. `ChildProcess` extends `EventEmitter`
    #+BEGIN_SRC js
    import { spawn } from "node:child_process"
    const ls = spawn("ls", ["-lah", "/usr/lib"])
    ls.on("error", console.error)
    ls.on("close", exitCode => console.log(exitCode))
    ls.stdout.setEncoding("utf8")
    ls.stdout.on("data", console.log)
    ls.stderr.setEncoding("utf8")
    ls.stderr.on("data", console.error)
    #+END_SRC
- `exec(cmd, opts, done): ChildProcess` executes a *shell command*
    #+BEGIN_SRC js
    import { exec } from "node:child_process"
    exec("for x in a b c; do echo $x; done", (err, stdout, stderr) => {
      if (err) { return console.log(err) }
      console.log(stdout) // a b c
    })
    #+END_SRC
    #+BEGIN_SRC js
    import { promisify } from "node:util"
    import { exec } from "node:child_process"
    const execp = promisify(exec)
    try {
      const { stdout, stderr } =
            await execp("echo a b c | tr '[:lower:]' '[:upper:]'")
      console.log(stdout) // A B C
    } catch (err) { console.error(err) }
    #+END_SRC
- `execFile(file, args, opts, done): ChildProcess` executes a *command from a
   file* without a shell
    #+BEGIN_SRC js
    import { execFile } from "node:child_process"
    execFile("/usr/bin/bun", ["--version"], (err, stdout, stderr) => {
      if (err) { return console.log(err) }
      console.log(stdout) // 1.0.7
    })
    #+END_SRC
- `fork(module, args, opts): ChildProcess` forks a *Node.js child process*
  with a bidirectional IPC channel
    #+BEGIN_SRC js
    import { fork } from "node:child_process"
    const [_, file, args] = process.argv
    if (args === "child") { // child
      // recieve a signal and close the IPC channel with a parent
      process.on("SIGUSR2", process.disconnect)
      process.on("message", msg => { // receive a message from a parent
        console.log("chd", msg)
        process.send({ res: "chd => par" }) // send a amessage to a parent
      })
    } else { // parent
      const child = fork(file, ["child"]) // fork a Node.js child process
      child.on("error", err => console.error("par", err))
      child.on("close", exitCode => console.log("par", exitCode))
      // send a message to a child once a child is spawned
      child.on("spawn", () => child.send({ req: "par => chd" }))
      child.on("message", msg => { // receive a message from a child
        console.log("par", msg)
        child.kill("SIGUSR2") // send a signar to a child
      })
    }
    #+END_SRC

** Worker thread

- `new Worker(module, opts)` creates an independent, dedicated *thread for
  CPU-bound tasks* executed in parallel with the main thread event loop. A
  Worker has an async *bidirectional communication channel* with its parent
  `parentPort/worker.postMessage()/.on(message)`, per-thread *own event loop*,
  and a separate V8 instance (small memory footprint, fast startup time, safe:
  no synchronization, *no resource sharing*, no race conditions). All
  communication over a channel between a parent and a Worker is serialized and
  deserialized
    #+BEGIN_SRC js
    import {
      Worker, isMainThread, parentPort, workerData,
      setEnvironmentData, getEnvironmentData
    } from "node:worker_threads"
    const [_, file] = process.argv
    if (isMainThread) { // main thread
      // parameterize a worker before creation of a worker
      setEnvironmentData("worker.inc", 3);
      // provide a workload for a worker
      const worker = new Worker(file, { workerData: [1, 2, 3] })
      worker.on("error", error => console.error("main", error))
      worker.on("exit", exitCode => console.log("main", exitCode))
      // receive a message from a worker
      worker.on("message", msg => console.log("main", msg))
      // send a message to a worker
      setTimeout(() => worker.postMessage({ req: "start" }), 100)
    } else { // worker thread
      const inc = getEnvironmentData("worker.inc") // get a clone of parameters
      console.log("work", workerData, inc)
      // receive a clone of a message from a parent
      parentPort.on("message", msg => {
        console.log("work", msg)
        // process a clone of a workload and send a message to a parent
        parentPort.postMessage(workerData.map(el => el + inc))
        parentPort.close() // close an async bidirectional channel
      })
    }
    #+END_SRC
- A custom `MessageChannel` can be created on either thread for separation of
  concerns and one of the `pors1/port2`s can be passed to the other thread over
  the default channel
    #+BEGIN_SRC js
    import { Worker, isMainThread, parentPort, MessageChannel }
    from "node:worker_threads"
    const [node, file] = process.argv
    if (isMainThread) {
      const worker = new Worker(file)
      worker.on("error", error => console.error("main", error))
      worker.on("exit", exitCode => console.log("main", exitCode))
      // create a new dedicate channel for separation of concerns
      const newChannel = new MessageChannel()
      // send a new channel to a worker
      worker.postMessage({ port: newChannel.port1 }, [newChannel.port1])
      // receive a message from a worker on a new channel
      newChannel.port2.on("message", msg => console.log("main", msg))
    } else {
      let newPort
      parentPort.once("message", msg => {
        // receive a new channel from a parent
        const { port: newPort } = msg
        // send a message to a parent over a new channel
        newPort.postMessage({ worker: "hi" })
        newPort.close()
      })
    }
    #+END_SRC
